{
 "metadata": {
  "name": "ml_COMS4721_hw1_mayank_misra_mm3557"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "# Name:  Mayank Misra | Uni: mm3557 | ml_COMS4721_hw1_mayank_misra_mm3557"
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "#Preliminaries"
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "### Question 1: True or False?\n#### 1. Supervised and unsupervised learning both aim to identify classes in data (at the learning stage not prediction).\n####    Answer>>> True\n#### 2. When feature space is large, overfitting is likely.\n####    Answer>>> True\n#### 3. Overfitting can be controlled by regularization.\n####    Answer>>> True\n#### 4. Once you learn a classification model, you can use the test set to assess the model performance.\n####    Answer>>> True\n#### 5. If the performance of a classification model on the test set is poor, you can re-calibrate your model parameters to achieve a better model.\n####    Answer>>> False\n#### 6. Cross-validation is used only when one have a large training set.\n####    Answer>>> False\n#### 7. The examples in a validation set are used to train a classification model.\n####    Answer>>> False\n#### 8. To learn a regression model you can either use gradient descent or normal equations.\n####    Answer>>> True\n#### 9. Because it is straightforward to calculate in just one step, Normal equation is the preferred method when the feature space is large (e.g., 10,000 features).\n####    Answer>>> False\n#### 10. If the learning rate \u03b1 is small enough, gradient descent converges very fast.\n####    Answer>>> False\n#### 11. Ridge regression aims to increase the variance of linear regression by decreasing the bias.\n####    Answer>>> False\n#### 12. Lasso is a variant of linear regression that calculates a sparse solution.\n####    Answer>>> True\n#### 13. K-NN works only for classification.\n####    Answer>>> False\n#### 14. K-NN is a linear classification method.\n####    Answer>>> False\n#### 15. The loss function aggregates the classification/regression error on all examples.\n####    Answer>>> True"
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "### Question 2: Machine Learning Definition in Practice\n#### What are the sets E, T, and P in the case of a Recommender System? Please justify your answer and elaborate with examples.\n\n#### Recall: \u201cA computer program is said to learn from experience E with respect to some class of tasks T and performance measure P, if its performance at tasks in T, as measured by P, improves with experience E. \u201d Tom Mitchell.\n\n####   Answer>>> Recommendation systems are a subclass of information filtering system that seek to predict the 'rating' or 'preference' that user would give to an item. (Francesco Ricci and Lior Rokach and Bracha Shapira, Introduction to Recommender Systems Handbook, Recommender Systems Handbook, Springer, 2011, pp. 1-35.  http://www.inf.unibz.it/~ricci/papers/intro-rec-sys-handbook.pdf) \n\n####   In a recommendation system, the available data on the user (user matrix of purchase history, likes/dislikes, ratings, surveys, profile) is mined to predict future behavior (buying pattern, product preferences) in attempt to provide a contextual and personalized experience.  \n\n####   Experience E:  \n####   The trainging Experience is the learning simulations where predictions from the model are matched with known results. For example, a set of movie likes/dislikes can be used to model the probability of the user liking a particular movie. These experiences are used to further refine the model to make it better (it learns form its experiences).  \n\n####   Tasks T:  \n####   The act of predicting/recommending a specific user behavior is the task.   \n\n####   Performance P:  \n####   Performance of a recommendation systemis the accuracy with which the underlying model is able to predict the actual behaviour of a user, given the training datset (of user preferences and past behaviors).  This result will feedback into the user matrix and will become part of future predictions therby improving the acurracy of the model.   The training dataset is the past data on users that captures categorical and descriptive information (like/dislike, age, purchases etc)."
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "# Practical Problems"
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "## 1. Linear regression with one feature\n#### We are interested in studying the relationship between age and height (statures) in girls aged 2 to 8 years old. We think that this can be modeled with a linear regression model. We have examples (data points) for a population of 60 girls. Each example has one feature Age along with a numerical label Height. We will use the dataset girls_train.csv (derived from CDC growthchart data1). Your mission is to implement linear regression with one feature using gradient descent. You will plot the data, regression line, coefficient contours and cost function. You will finally make a prediction for a new example using your regression model and assess your out of sample mean square error."
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "### 1.1 Load & Plot\n#### Load the dataset girls.csv. and plot the distribution of the data. \n####The plot of girls_train.csv looks like this:  [Age and Stature](https://pbs.twimg.com/media/BiUfDq5IgAEgENU.png)"
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "import numpy as np\n# include numpy libraries to facilitate reading text files like csv\nimport matplotlib as mpl\n# include the matplot libraries\nimport matplotlib.pyplot as plt\n# include plotting libraries\n\ndef getData():\n    # specify the data file that will be read\n    age, height = np.loadtxt('/Users/mayank/Dropbox/DataSets/ColumbiaUniversity/ML/hw1_all/girls_train.csv', \n                             dtype='float', \n                             comments='# The file contains a null third row.  Force loadtxt to read first two columns', \n                             delimiter=',', \n                             converters=None, \n                             skiprows=0, \n                             usecols=(0,1), \n                             unpack=True, \n                             ndmin=0)\n    print (age)\n    print (height)\n    \n    fig = plt.figure()\n    axl = fig.add_subplot(1,1,1,axisbg='white')\n    plt.plot(age, height, 'ro')\n    plt.title('Age and Stature')\n    plt.xlabel('Age in Years')\n    plt.ylabel('Height in Meters')\n    plt.show()\n\ngetData()\n\n",
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": "[ 2.    2.04  2.21  2.38  2.46  2.54  2.63  2.79  2.88  2.96  3.04  3.13\n  3.21  3.29  3.38  3.54  3.63  3.71  3.88  3.96  4.29  4.38  4.46  4.54\n  4.63  4.71  4.88  4.96  5.04  5.13  5.21  5.29  5.38  5.46  5.54  5.63\n  5.71  5.79  5.88  5.96  6.04  6.13  6.21  6.38  6.54  6.63  6.79  6.88\n  6.96  7.04  7.13  7.21  7.29  7.38  7.63  7.71  7.79  7.88  7.96  8.04]\n[ 0.87311214  0.87749175  0.87157142  0.94899744  0.89584767  0.95190833\n  0.93627413  0.89800446  0.99528907  0.96300288  0.96900715  0.92122983\n  0.87959451  0.98705683  0.99299569  0.97648481  1.01069581  1.01657883\n  1.02835032  1.07791928  1.0861266   1.00310761  0.96651891  0.98747315\n  1.11111413  1.02532875  1.03654885  1.04218237  1.14270129  1.14905168\n  1.05915598  1.02011035  1.16812988  1.03094842  1.18084504  1.1871931\n  1.04719061  1.06964808  1.20616342  1.05232262  1.21871833  1.24546059\n  1.06813207  1.08987746  1.10028513  1.12348325  1.20125449  1.30110291\n  1.28534483  1.31293576  1.29685453  1.22827864  1.13296343  1.27753855\n  1.18288605  1.22075253  1.16001227  1.17687516  1.35092955  1.35601536]\n"
      }
     ],
     "prompt_number": 1
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "####The plot of girls_train.csv looks like this:  [Age and Stature](https://pbs.twimg.com/media/BiUfDq5IgAEgENU.png)"
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "### 1.2 Gradient descent\n#### The \u03b2\u2019s of the model with a learning rate alpha = 0.05 and #iterations = 1500.\n#### The mean square error of your regression model on the training set?\n"
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "#Summary\n#Load the dataset - csv with no headers\n#Initialize useful parameters for regression\n#Compute gradient descent (GD) to infer beta. GD steps = numberIterations  and learning rate = alpha\n#initialize gradient descent parameters\n#compute and display initial cost\n#Predict height for girl aged 4.5\n#code ported from http://bit.ly/1iu3Wke (based on Ex1 from Andrew Ng Coursera ml-class.org)\n# include numpy libraries to facilitate reading text files like csv\nimport numpy as np\nfrom numpy import loadtxt, zeros, ones, array, linspace, logspace\n# include the matplot libraries\nimport matplotlib as mpl\n# include plotting libraries\nimport matplotlib.pyplot as plt\nfrom pylab import scatter, show, title, xlabel, ylabel, plot, contour\n\n#Load the dataset\ndata = loadtxt('/Users/mayank/Dropbox/DataSets/ColumbiaUniversity/ML/hw1_all/girls_train.csv',\n                             dtype='float', \n                             comments='# The file contains a null third row.  Force loadtxt to read first two columns', \n                             delimiter=',', \n                             #converters=None, \n                             #skiprows=0, \n                             usecols=(0,1), \n                             unpack=True, \n                             ndmin=0\n                             ) \n \n#Initialize useful parameters for regression \ndef getCost(X, y, beta):\n\n    #Number of training samples\n    m = y.size\n \n    #calculate the cost of a particular choice of beta\n    predictedValue = X.dot(beta).flatten()\n \n    #calculate squared error\n    sqErrors = (predictedValue - y) ** 2\n    \n    J = (1.0 / (2 * m)) * sqErrors.sum()\n    \n    return J\n \n#Compute gradient descent (GD) to infer beta. GD steps = numberIterations  and learning rate = alpha\ndef gradientDescent(X, y, beta, alpha, numberIterations):\n    # number of training examples\n    m = y.size\n    J_history = zeros(shape=(numberIterations, 1))\n \n    for i in range(numberIterations):\n \n        predictedValue = X.dot(beta).flatten()\n \n        sumErrorsCol1 = (predictedValue - y) * X[:, 0]\n        sumErrorsCol2 = (predictedValue - y) * X[:, 1]\n \n        beta[0][0] = beta[0][0] - alpha * (1.0 / m) * sumErrorsCol1.sum()\n        beta[1][0] = beta[1][0] - alpha * (1.0 / m) * sumErrorsCol2.sum()\n \n        J_history[i, 0] = getCost(X, y, beta)\n \n    return beta, J_history\n \nX = data[:, 0]\ny = data[:, 1]\n \n \n#number of training samples\nm = y.size\n \n#Add a column of ones to X \nX1 = ones(shape=(m, 2))\nX1[:, 1] = X\n \n#Initialize beta parameters\nbeta = zeros(shape=(2, 1))\n \n#initialize gradient descent parameters\nnumberIterations = 1500\nalpha = 0.05\n \n#compute and display initial cost\nprint 'Initial cost is %f' % getCost(X1, y, beta)\n \nbeta, J_history = gradientDescent(X1, y, beta, alpha, numberIterations)\nprint beta\n\n",
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": "Initial cost is 1.232898\n[[-0.02288278]\n [ 1.03139807]]"
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": "\n"
      }
     ],
     "prompt_number": 36
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "### 1.3 Plot the regression line, contours and bowl function"
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "#### [Link to the resulting Contour Plot](https://pbs.twimg.com/media/BibJ88pIgAACG_k.png)"
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "#Summary\n#Load the dataset - csv with no headers\n#Initialize useful parameters for regression\n#Compute gradient descent (GD) to infer beta. GD steps = numberIterations  and learning rate = alpha\n#initialize gradient descent parameters\n#compute and display initial cost\n#Predict height for girl aged 4.5\n#code ported from http://bit.ly/1iu3Wke (based on Ex1 from Andrew Ng Coursera ml-class.org)\n# include numpy libraries to facilitate reading text files like csv\nimport numpy as np\nfrom numpy import loadtxt, zeros, ones, array, linspace, logspace\n# include the matplot libraries\nimport matplotlib as mpl\n# include plotting libraries\nimport matplotlib.pyplot as plt\nfrom pylab import scatter, show, title, xlabel, ylabel, plot, contour\n\n#Load the dataset\ndata = loadtxt('/Users/mayank/Dropbox/DataSets/ColumbiaUniversity/ML/hw1_all/girls_train.csv',\n                             dtype='float', \n                             comments='# The file contains a null third row.  Force loadtxt to read first two columns', \n                             delimiter=',', \n                             #converters=None, \n                             #skiprows=0, \n                             usecols=(0,1), \n                             unpack=True, \n                             ndmin=0\n                             ) \n \n#Initialize useful parameters for regression \ndef getCost(X, y, beta):\n\n    #Number of training samples\n    m = y.size\n \n    #calculate the cost of a particular choice of beta\n    predictedValue = X.dot(beta).flatten()\n \n    #calculate squared error\n    sqErrors = (predictedValue - y) ** 2\n    \n    J = (1.0 / (2 * m)) * sqErrors.sum()\n \n    return J\n \n#Compute gradient descent (GD) to infer beta. GD steps = numberIterations  and learning rate = alpha\ndef gradientDescent(X, y, beta, alpha, numberIterations):\n    # number of training examples\n    m = y.size\n    J_history = zeros(shape=(numberIterations, 1))\n \n    for i in range(numberIterations):\n \n        predictedValue = X.dot(beta).flatten()\n \n        sumErrorsCol1 = (predictedValue - y) * X[:, 0]\n        sumErrorsCol2 = (predictedValue - y) * X[:, 1]\n \n        beta[0][0] = beta[0][0] - alpha * (1.0 / m) * sumErrorsCol1.sum()\n        beta[1][0] = beta[1][0] - alpha * (1.0 / m) * sumErrorsCol2.sum()\n \n        J_history[i, 0] = getCost(X, y, beta)\n \n    return beta, J_history\n \nX = data[:, 0]\ny = data[:, 1]\n \n \n#number of training samples\nm = y.size\n \n#Add a column of ones to X \nX1 = ones(shape=(m, 2))\nX1[:, 1] = X\n \n#Initialize beta parameters\nbeta = zeros(shape=(2, 1))\n \n#initialize gradient descent parameters\nnumberIterations = 1500\nalpha = 0.05\n \n#compute beta\n \nbeta, J_history = gradientDescent(X1, y, beta, alpha, numberIterations)\n\n\n\n## PLOTS \n\n#Plot the data\ndef getData():\n    # specify the data file that will be read\n    age, height = np.loadtxt('/Users/mayank/Dropbox/DataSets/ColumbiaUniversity/ML/hw1_all/girls_train.csv', \n                             dtype='float', \n                             comments='# The file contains a null third row.  Force loadtxt to read first two columns', \n                             delimiter=',', \n                             converters=None, \n                             skiprows=0, \n                             usecols=(0,1), \n                             unpack=True, \n                             ndmin=0)\n    #y = -0.02288278 + 1.03139807*x\n    fig = plt.figure()\n    axl = fig.add_subplot(1,1,1,axisbg='white')\n    plt.plot(age, height, 'ro')\n    plt.title('Age and Stature')\n    plt.xlabel('Age in Years')\n    plt.ylabel('Height in Meters')\n    plt.show()\n\ngetData()\n\n#Plot the results\nresult = X1.dot(beta).flatten()\nplot(data[:, 1], result)\nshow()\n \n#plot values of beta\n#initialize matrix space for beta and cost values.  \nbeta0 = linspace(-10, 10, 100)\nbeta1 = linspace(-1, 4, 100)\nJ_beta0beta1 = zeros(shape=(beta0.size, beta1.size))\n#Fill the matrix J_beta0beta1\nfor t1, element in enumerate(beta0):\n    for t2, element2 in enumerate(beta1):\n        iBeta = zeros(shape=(2, 1))\n        iBeta[0][0] = element\n        iBeta[1][0] = element2\n        J_beta0beta1[t1, t2] = getCost(X1, y, iBeta)\n \n#graph a contour plot\nJ_beta0beta1 = J_beta0beta1.T\ncontour(beta0, beta1, J_beta0beta1, logspace(-2, 3, 20))\nxlabel('beta_0')\nylabel('beta_1')\nscatter(beta[0][0], beta[1][0])\nshow()",
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 8
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "#### [Contour Plot](https://pbs.twimg.com/media/BibJ88pIgAACG_k.png)"
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "### 1.4 Testing your model and Making a prediction for a new example"
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "#### For a girl of age = 4.5, the prediction for height is 0.928426"
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "#Summary\n#Load the dataset - csv with no headers\n#Initialize useful parameters for regression\n#Compute gradient descent (GD) to infer beta. GD steps = numberIterations  and learning rate = alpha\n#initialize gradient descent parameters\n#compute and display initial cost\n#Predict height for girl aged 4.5\n#code ported from http://bit.ly/1iu3Wke (based on Ex1 from Andrew Ng Coursera ml-class.org)\n# include numpy libraries to facilitate reading text files like csv\nimport numpy as np\nfrom numpy import loadtxt, zeros, ones, array, linspace, logspace\n# include the matplot libraries\nimport matplotlib as mpl\n# include plotting libraries\nimport matplotlib.pyplot as plt\nfrom pylab import scatter, show, title, xlabel, ylabel, plot, contour\n\n#Load the dataset\ndata = loadtxt('/Users/mayank/Dropbox/DataSets/ColumbiaUniversity/ML/hw1_all/girls_train.csv',\n                             dtype='float', \n                             comments='# The file contains a null third row.  Force loadtxt to read first two columns', \n                             delimiter=',', \n                             #converters=None, \n                             #skiprows=0, \n                             usecols=(0,1), \n                             unpack=True, \n                             ndmin=0\n                             ) \n \n#Initialize useful parameters for regression \ndef getCost(X, y, beta):\n\n    #Number of training samples\n    m = y.size\n \n    #calculate the cost of a particular choice of beta\n    predictedValue = X.dot(beta).flatten()\n \n    #calculate squared error\n    sqErrors = (predictedValue - y) ** 2\n    \n    J = (1.0 / (2 * m)) * sqErrors.sum()\n \n    return J\n \n#Compute gradient descent (GD) to infer beta. GD steps = numberIterations  and learning rate = alpha\ndef gradientDescent(X, y, beta, alpha, numberIterations):\n    # number of training examples\n    m = y.size\n    J_history = zeros(shape=(numberIterations, 1))\n \n    for i in range(numberIterations):\n \n        predictedValue = X.dot(beta).flatten()\n \n        sumErrorsCol1 = (predictedValue - y) * X[:, 0]\n        sumErrorsCol2 = (predictedValue - y) * X[:, 1]\n \n        beta[0][0] = beta[0][0] - alpha * (1.0 / m) * sumErrorsCol1.sum()\n        beta[1][0] = beta[1][0] - alpha * (1.0 / m) * sumErrorsCol2.sum()\n \n        J_history[i, 0] = getCost(X, y, beta)\n \n    return beta, J_history\n \nX = data[:, 0]\ny = data[:, 1]\n \n \n#number of training samples\nm = y.size\n \n#Add a column of ones to X \nX1 = ones(shape=(m, 2))\nX1[:, 1] = X\n \n#Initialize beta parameters\nbeta = zeros(shape=(2, 1))\n \n#initialize gradient descent parameters\nnumberIterations = 1500\nalpha = 0.05\n\nbeta, J_history = gradientDescent(X1, y, beta, alpha, numberIterations)\n\n\n#Predict height for girl aged 4.5\n\npredict1 = array([4.5, 1]).dot(beta).flatten()\nprint 'For a girl of age = 4.5, the prediction for height is %f' % (predict1)\n\n",
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": "For a girl of age = 4.5, the prediction for height is 0.928426\n"
      }
     ],
     "prompt_number": 41
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "### Compare mean sqaured error of test set with training set"
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "#### Initial cost for train set:  1.193600\n#### Initial cost for test set:  1.232898\n#### The beta values for train set:  -0.07334643 and 1.03658222\n#### The beta values for test set:  -0.02288278 and 1.03139807"
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "#Summary\n#Load the dataset - csv with no headers\n#Initialize useful parameters for regression\n#Compute gradient descent (GD) to infer beta. GD steps = numberIterations  and learning rate = alpha\n#initialize gradient descent parameters\n#compute and display initial cost\n#Predict height for girl aged 4.5\n#code ported from http://bit.ly/1iu3Wke (based on Ex1 from Andrew Ng Coursera ml-class.org)\n# include numpy libraries to facilitate reading text files like csv\nimport numpy as np\nfrom numpy import loadtxt, zeros, ones, array, linspace, logspace\n# include the matplot libraries\nimport matplotlib as mpl\n# include plotting libraries\nimport matplotlib.pyplot as plt\nfrom pylab import scatter, show, title, xlabel, ylabel, plot, contour\n\n#Load the dataset\ndata = loadtxt('/Users/mayank/Dropbox/DataSets/ColumbiaUniversity/ML/hw1_all/girls_test.csv',\n                             dtype='float', \n                             comments='# The file contains a null third row.  Force loadtxt to read first two columns', \n                             delimiter=',', \n                             #converters=None, \n                             #skiprows=0, \n                             usecols=(0,1), \n                             unpack=True, \n                             ndmin=0\n                             ) \n \n#Initialize useful parameters for regression \ndef getCost(X, y, beta):\n\n    #Number of training samples\n    m = y.size\n \n    #calculate the cost of a particular choice of beta\n    predictedValue = X.dot(beta).flatten()\n \n    #calculate squared error\n    sqErrors = (predictedValue - y) ** 2\n    \n    J = (1.0 / (2 * m)) * sqErrors.sum()\n    \n    return J\n \n#Compute gradient descent (GD) to infer beta. GD steps = numberIterations  and learning rate = alpha\ndef gradientDescent(X, y, beta, alpha, numberIterations):\n    # number of training examples\n    m = y.size\n    J_history = zeros(shape=(numberIterations, 1))\n \n    for i in range(numberIterations):\n \n        predictedValue = X.dot(beta).flatten()\n \n        sumErrorsCol1 = (predictedValue - y) * X[:, 0]\n        sumErrorsCol2 = (predictedValue - y) * X[:, 1]\n \n        beta[0][0] = beta[0][0] - alpha * (1.0 / m) * sumErrorsCol1.sum()\n        beta[1][0] = beta[1][0] - alpha * (1.0 / m) * sumErrorsCol2.sum()\n \n        J_history[i, 0] = getCost(X, y, beta)\n \n    return beta, J_history\n \nX = data[:, 0]\ny = data[:, 1]\n \n \n#number of training samples\nm = y.size\n \n#Add a column of ones to X \nX1 = ones(shape=(m, 2))\nX1[:, 1] = X\n \n#Initialize beta parameters\nbeta = zeros(shape=(2, 1))\n \n#initialize gradient descent parameters\nnumberIterations = 1500\nalpha = 0.05\n \n#compute and display initial cost\nprint 'Initial cost is %f' % getCost(X1, y, beta)\n \nbeta, J_history = gradientDescent(X1, y, beta, alpha, numberIterations)\nprint beta\n",
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": "Initial cost is 1.193600\n[[-0.07334643]\n [ 1.03658222]]"
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": "\n"
      }
     ],
     "prompt_number": 56
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "#### Initial cost for train set:  1.193600\n#### Initial cost for test set:  1.232898\n#### The beta values for train set:  -0.07334643 and 1.03658222\n#### The beta values for test set:  -0.02288278 and 1.03139807"
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "## 2. Linear regression with multiple features"
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "#### In this problem, you will work on linear regression with multiple features using gradient descent and the normal equation. You will also study the relationship between the risk function, the convergence of gradient descent, and the learning rate. We will use the dataset girls age weight height 2 8.csv (derived from CDC growthchart data)."
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "#### 2.1 Data Preparation & Normalization\n#### 2.2 Multi variable Gradient Descent on normalized data"
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "#### Plot the data for exploration and size\n\n#### [Link to the 3d distribution of data](https://github.com/mayankmisra/mayankmisra.github.io/blob/rel-0.2/assets/Age-Weight-Height-ExploreData.png?raw=true)"
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "import numpy as np\n# include numpy libraries to facilitate reading text files like csv\nimport matplotlib as mpl\n# include the matplot libraries\nimport matplotlib.pyplot as plt\n# include plotting libraries\n\ndef getData():\n    # specify the data file that will be read\n    age, weight, height = np.loadtxt('/Users/mayank/Dropbox/DataSets/ColumbiaUniversity/ML/hw1_all/girls_age_weight_height_2_8.csv',\n                             dtype='float', \n                             #comments='#', \n                             delimiter=',', \n                             #converters=None, \n                             #skiprows=0, \n                             #usecols=(0,1), \n                             unpack=True, \n                             ndmin=0\n                             )\n    print (age)\n    print (weight)\n    print (height)\n    \n    fig = plt.figure()\n    ax = fig.gca(projection=\"3d\")\n    plt.plot(age, weight, height, 'ro')\n    plt.title('Age Weight Height')\n    ax.set_xlabel('Age in Years')\n    ax.set_ylabel('Weight in Kilograms')\n    ax.set_zlabel('Height in Meters')\n    plt.show()\n\ngetData()",
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": "[ 2.    2.04  2.13  2.21  2.29  2.38  2.46  2.54  2.63  2.71  2.79  2.88\n  2.96  3.04  3.13  3.21  3.29  3.38  3.46  3.54  3.63  3.71  3.79  3.88\n  3.96  4.04  4.13  4.21  4.29  4.38  4.46  4.54  4.63  4.71  4.79  4.88\n  4.96  5.04  5.13  5.21  5.29  5.38  5.46  5.54  5.63  5.71  5.79  5.88\n  5.96  6.04  6.13  6.21  6.29  6.38  6.46  6.54  6.63  6.71  6.79  6.88\n  6.96  7.04  7.13  7.21  7.29  7.38  7.46  7.54  7.63  7.71  7.79  7.88\n  7.96  8.04  8.13  8.21  8.29  8.38  8.46]\n[ 10.21027  13.07613  11.44697  14.43984  12.59622  10.5199   12.89517\n  12.11692  16.76085  11.20934  13.48913  11.85574  11.54332  12.90222\n  13.03542  11.88202  11.99685  11.82981  12.70158  19.58748  16.46093\n  15.20721  15.37263  14.29485  13.47689  13.61116  13.21864  13.02441\n  18.04961  18.25533  13.40907  15.51193  15.66975  17.28859  14.29081\n  21.63373  14.20687  14.34277  20.16834  25.58315  18.58571  14.8925\n  16.06749  15.56413  25.83467  17.81035  15.58975  16.83304  15.87089\n  16.43608  22.90029  28.0358   29.97981  26.52102  21.35797  17.3225\n  29.70296  30.04782  20.11027  18.73445  22.64564  18.22805  18.38153\n  32.20984  18.16912  19.73432  19.00511  27.35114  22.04564  19.48502\n  19.64775  19.23024  20.96755  33.19435  20.31464  29.81185  20.65887\n  26.82559  40.94614]\n[ 0.8052476  0.9194741  0.9083505  0.8037555  0.811357   0.9489974\n  0.9664505  0.9288403  0.86205    0.9811632  0.9883778  1.004748   1.001915\n  0.9934899  0.8974875  0.8887256  0.932307   0.937784   1.05032    1.056727\n  0.9821247  0.91031    1.065316   1.02835    0.9255748  0.9306862\n  1.101614   0.9921132  1.114548   1.063936   1.008634   1.044635   1.140624\n  0.9824303  1.062029   1.100134   1.166945   1.161301   1.009289   1.091316\n  1.097202   1.025529   1.076194   1.114876   1.187193   1.09323    1.069648\n  1.173466   1.232684   1.057615   1.245461   1.251804   1.167406   1.089877\n  1.215447   1.118232   1.123483   1.110545   1.309295   1.108909   1.175754\n  1.149112   1.135427   1.339077   1.330256   1.313696   1.173512   1.159026\n  1.367356   1.168069   1.263869   1.368565   1.388876   1.318567   1.384097\n  1.213855   1.218047   1.414521   1.301148 ]\n"
      }
     ],
     "prompt_number": 42
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "#### Data Preparation, Normalization and Gradient Descent"
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "#Summary\n#Load the dataset - csv with no headers\n#Initialize useful parameters for regression\n#Compute gradient descent (GD) to infer beta. GD steps = numberIterations  and learning rate = alpha\n#initialize gradient descent parameters\n#compute and display initial cost\n#Predict height for girl aged 4.5\n#code ported from http://bit.ly/1fnpcVn (based on Andrew Ng Coursera ml-class.org)\n# include numpy libraries to facilitate reading text files like csv\nfrom numpy import loadtxt, zeros, ones, array, linspace, logspace, mean, std, arange\nfrom mpl_toolkits.mplot3d import Axes3D\nimport matplotlib.pyplot as plt\nfrom pylab import plot, show, xlabel, ylabel\n\n#Load the dataset\ndata = loadtxt('/Users/mayank/Dropbox/DataSets/ColumbiaUniversity/ML/hw1_all/girls_age_weight_height_2_8.csv',\n                             dtype='float', \n                             #comments='#', \n                             delimiter=',', \n                             #converters=None, \n                             #skiprows=0, \n                             #usecols=(0,1), \n                             unpack=True, \n                             ndmin=0\n                             )\n                             \n#Normalize features- implies regress data so that mean is 0 and standard deviation is 1.  \ndef normalizeFeatures(X):\n    normalizedStdDev = []\n    normalizedMean = []\n \n    normalizeX = X\n \n    rangeX = X.shape[1]\n    for i in range(rangeX):\n        getStdDev = std(X[:, i])\n        getMean = mean(X[:, i])\n        normalizedStdDev.append(getStdDev)\n        normalizedMean.append(getMean)\n        normalizeX[:, i] = (normalizeX[:, i] - getMean) / getStdDev\n \n    return normalizeX, normalizedMean, normalizedStdDev\n \n#Initialize useful parameters for regression \ndef getCost(X, y, beta):\n    #calculate training examples\n    m = y.size\n    #calculate the cost of a particular choice of beta\n    predictedValue = X.dot(beta)\n    #calculate squared error\n    sqErrors = (predictedValue - y)\n \n    J = (1.0 / (2 * m)) * sqErrors.T.dot(sqErrors)\n \n    return J\n\n#Compute gradient descent (GD) to infer beta. GD steps = numberIterations  and learning rate = alpha \ndef gradientDescent(X, y, beta, alpha, numberIterations):\n    #number of training examples\n    m = y.size\n    J_history = zeros(shape=(numberIterations, 1))\n \n    for i in range(numberIterations):\n        predictedValue = X.dot(beta) \n        betaSize = beta.size\n \n        for X1 in range(betaSize):\n \n            hold = X[:, X1]\n            hold.shape = (m, 1)\n            #calculate step errors\n            sumErrorsCol = (predictedValue - y) * hold\n            #calculate step beta values\n            beta[X1][0] = beta[X1][0] - alpha * (1.0 / m) * sumErrorsCol.sum()\n \n        J_history[i, 0] = getCost(X, y, beta)\n \n    return beta, J_history\n \n#Initialize GD regression and print risk/cost\nX = data[:, :2]\ny = data[:, 2]\n \n#number of training samples\nm = y.size\n\ny.shape = (m, 1)\n \n#Normalize features- implies regress data so that mean is 0 and standard deviation is 1. \nx, normalizedMean, normalizedStdDev = normalizeFeatures(X)\n \n#Add a column of ones to X\nX1 = ones(shape=(m, 3))\nX1[:, 1:3] = x\n \n#initialize gradient descent parameters\niterations = 50\nalpha = 0.001\n \n#Initialize beta parameters\nbeta = zeros(shape=(3, 1))\n \nbeta, J_history = gradientDescent(X1, y, beta, alpha, iterations)\nprint beta, J_history\n",
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": "[[ 0.2356007 ]\n [ 0.22414601]\n [ 0.22409914]] [[ 22.66497412]\n [ 22.5976511 ]\n [ 22.53055061]\n [ 22.46367186]\n [ 22.39701405]\n [ 22.33057639]\n [ 22.2643581 ]\n [ 22.19835839]\n [ 22.13257648]\n [ 22.06701159]\n [ 22.00166294]\n [ 21.93652977]\n [ 21.8716113 ]\n [ 21.80690677]\n [ 21.74241541]\n [ 21.67813647]\n [ 21.61406918]\n [ 21.55021278]\n [ 21.48656653]\n [ 21.42312968]\n [ 21.35990148]\n [ 21.29688118]\n [ 21.23406805]\n [ 21.17146134]\n [ 21.10906032]\n [ 21.04686426]\n [ 20.98487243]\n [ 20.9230841 ]\n [ 20.86149854]\n [ 20.80011503]\n [ 20.73893286]\n [ 20.6779513 ]\n [ 20.61716965]\n [ 20.55658719]\n [ 20.49620321]\n [ 20.43601701]\n [ 20.37602788]\n [ 20.31623513]\n [ 20.25663805]\n [ 20.19723596]\n [ 20.13802815]\n [ 20.07901394]\n [ 20.02019264]\n [ 19.96156356]\n [ 19.90312602]\n [ 19.84487935]\n [ 19.78682286]\n [ 19.72895589]\n [ 19.67127775]\n [ 19.61378778]]\n"
      }
     ],
     "prompt_number": 44
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "#### 2.3 Plotting Risk function for different learning rates"
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "#### Run gradient descent and plot the Risk function with respect to the number of iterations for different values of \u03b1 \u2208 {0.005, 0.001, 0.05, 0.1, 0.5, 1}"
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "#Summary\n#Load the dataset - csv with no headers\n#Initialize useful parameters for regression\n#Compute gradient descent (GD) to infer beta. GD steps = numberIterations  and learning rate = alpha\n#initialize gradient descent parameters\n#compute and display initial cost\n#Predict height for girl aged 4.5\n#code ported from http://bit.ly/1fnpcVn (based on Andrew Ng Coursera ml-class.org)\n# include numpy libraries to facilitate reading text files like csv\nfrom numpy import loadtxt, zeros, ones, array, linspace, logspace, mean, std, arange\nfrom mpl_toolkits.mplot3d import Axes3D\nimport matplotlib.pyplot as plt\nfrom pylab import plot, show, xlabel, ylabel\n\n#Load the dataset\ndata = loadtxt('/Users/mayank/Dropbox/DataSets/ColumbiaUniversity/ML/hw1_all/girls_age_weight_height_2_8.csv',\n                             dtype='float', \n                             #comments='#', \n                             delimiter=',', \n                             #converters=None, \n                             #skiprows=0, \n                             #usecols=(0,1), \n                             unpack=True, \n                             ndmin=0\n                             )\n                             \n#Normalize features- implies regress data so that mean is 0 and standard deviation is 1.\n#For the each feature x (a column in the data matrix)xscaled = (x \u2212 \u03bc(x))/(stdev(x))   \ndef normalizeFeatures(X):\n    normalizedStdDev = []\n    normalizedMean = []\n \n    normalizeX = X\n \n    rangeX = X.shape[1]\n    for i in range(rangeX):\n        getStdDev = std(X[:, i])\n        getMean = mean(X[:, i])\n        normalizedStdDev.append(getStdDev)\n        normalizedMean.append(getMean)\n        normalizeX[:, i] = (normalizeX[:, i] - getMean) / getStdDev\n \n    return normalizeX, normalizedMean, normalizedStdDev\n \n#Initialize useful parameters for regression \ndef getCost(X, y, beta):\n    #calculate training examples\n    m = y.size\n    #calculate the cost of a particular choice of beta\n    predictedValue = X.dot(beta)\n    #calculate squared error\n    sqErrors = (predictedValue - y)\n \n    J = (1.0 / (2 * m)) * sqErrors.T.dot(sqErrors)\n \n    return J\n\n#Compute gradient descent (GD) to infer beta. GD steps = numberIterations  and learning rate = alpha \ndef gradientDescent(X, y, beta, alpha, numberIterations):\n    #number of training examples\n    m = y.size\n    J_history = zeros(shape=(numberIterations, 1))\n \n    for i in range(numberIterations):\n        predictedValue = X.dot(beta) \n        betaSize = beta.size\n \n        for X1 in range(betaSize):\n \n            hold = X[:, X1]\n            hold.shape = (m, 1)\n            #calculate step errors\n            sumErrorsCol = (predictedValue - y) * hold\n            #calculate step beta values\n            beta[X1][0] = beta[X1][0] - alpha * (1.0 / m) * sumErrorsCol.sum()\n \n        J_history[i, 0] = getCost(X, y, beta)\n \n    return beta, J_history\n \n#Initialize regression and plot \nX = data[:, :2]\ny = data[:, 2]\n \n#number of training samples\nm = y.size\n\ny.shape = (m, 1)\n \n#Normalize features- implies regress data so that mean is 0 and standard deviation is 1. \nx, normalizedMean, normalizedStdDev = normalizeFeatures(X)\n \n#Add a column of ones to X\nX1 = ones(shape=(m, 3))\nX1[:, 1:3] = x\n \n#initialize gradient descent parameters\niterations = 50\nalpha = .05\n \n#Initialize beta parameters\nbeta = zeros(shape=(3, 1))\n \nbeta, J_history = gradientDescent(X1, y, beta, alpha, iterations)\n#print beta, J_history\nplot(arange(iterations), J_history)\nxlabel('Iterations')\nylabel('Cost Function')\nshow()\n",
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 46
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "#### [alpha = .001](https://github.com/mayankmisra/mayankmisra.github.io/blob/rel-0.2/assets/Age-Weight-Height-alpha001.png?raw=true)\n#### [alpha = .005](https://github.com/mayankmisra/mayankmisra.github.io/blob/rel-0.2/assets/Age-Weight-Height-alpha005.png?raw=true)\n#### [The best convergence is for alpha = .05](https://pbs.twimg.com/media/Biefh9UCEAE3SWH.png)\n#### [alpha = .5](https://github.com/mayankmisra/mayankmisra.github.io/blob/rel-0.2/assets/Age-Weight-Height-alphaDot5.png?raw=true)\n#### [alpha = 1](https://github.com/mayankmisra/mayankmisra.github.io/blob/rel-0.2/assets/Age-Weight-Height-alpha1.png?raw=true)"
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "#### 2.4 Normal equation  \u03b2 = (XtX)\u22121XT y\n##### Compare the \u03b2 vector you obtained with gradient descent to the one calculated with normal equation. Are they the same? Why?"
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "",
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "#### 2.5 Prediction"
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "##### a) Using both \u03b2 vectors (the one obtained with gradient descent and the one obtained with normal equations), make a height prediction for a 5-year old girl weighting 20 kilos (don\u2019t forget to scale!).\n##### b) Do gradient descent and Normal Equation lead to the same height prediction?"
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "#Summary\n#Load the dataset - csv with no headers\n#Initialize useful parameters for regression\n#Compute gradient descent (GD) to infer beta. GD steps = numberIterations  and learning rate = alpha\n#initialize gradient descent parameters\n#compute and display initial cost\n#Predict height for girl aged 4.5\n#code ported from http://bit.ly/1fnpcVn (based on Andrew Ng Coursera ml-class.org)\n# include numpy libraries to facilitate reading text files like csv\nfrom numpy import loadtxt, zeros, ones, array, linspace, logspace, mean, std, arange\nfrom mpl_toolkits.mplot3d import Axes3D\nimport matplotlib.pyplot as plt\nfrom pylab import plot, show, xlabel, ylabel\n\n#Load the dataset\ndata = loadtxt('/Users/mayank/Dropbox/DataSets/ColumbiaUniversity/ML/hw1_all/girls_age_weight_height_2_8.csv',\n                             dtype='float', \n                             #comments='#', \n                             delimiter=',', \n                             #converters=None, \n                             #skiprows=0, \n                             #usecols=(0,1), \n                             unpack=True, \n                             ndmin=0\n                             )\n                             \n#Normalize features- implies regress data so that mean is 0 and standard deviation is 1.\n#For the each feature x (a column in the data matrix)xscaled = (x \u2212 \u03bc(x))/(stdev(x))   \ndef normalizeFeatures(X):\n    normalizedStdDev = []\n    normalizedMean = []\n \n    normalizeX = X\n \n    rangeX = X.shape[1]\n    for i in range(rangeX):\n        getStdDev = std(X[:, i])\n        getMean = mean(X[:, i])\n        normalizedStdDev.append(getStdDev)\n        normalizedMean.append(getMean)\n        normalizeX[:, i] = (normalizeX[:, i] - getMean) / getStdDev\n \n    return normalizeX, normalizedMean, normalizedStdDev\n \n#Initialize useful parameters for regression \ndef getCost(X, y, beta):\n    #calculate training examples\n    m = y.size\n    #calculate the cost of a particular choice of beta\n    predictedValue = X.dot(beta)\n    #calculate squared error\n    sqErrors = (predictedValue - y)\n \n    J = (1.0 / (2 * m)) * sqErrors.T.dot(sqErrors)\n \n    return J\n\n#Compute gradient descent (GD) to infer beta. GD steps = numberIterations  and learning rate = alpha \ndef gradientDescent(X, y, beta, alpha, numberIterations):\n    #number of training examples\n    m = y.size\n    J_history = zeros(shape=(numberIterations, 1))\n \n    for i in range(numberIterations):\n        predictedValue = X.dot(beta) \n        betaSize = beta.size\n \n        for X1 in range(betaSize):\n \n            hold = X[:, X1]\n            hold.shape = (m, 1)\n            #calculate step errors\n            sumErrorsCol = (predictedValue - y) * hold\n            #calculate step beta values\n            beta[X1][0] = beta[X1][0] - alpha * (1.0 / m) * sumErrorsCol.sum()\n \n        J_history[i, 0] = getCost(X, y, beta)\n \n    return beta, J_history\n \n#Initialize regression and plot \nX = data[:, :2]\ny = data[:, 2]\n \n#number of training samples\nm = y.size\n\ny.shape = (m, 1)\n \n#Normalize features- implies regress data so that mean is 0 and standard deviation is 1. \nx, normalizedMean, normalizedStdDev = normalizeFeatures(X)\n \n#Add a column of ones to X\nX1 = ones(shape=(m, 3))\nX1[:, 1:3] = x\n \n#initialize gradient descent parameters\niterations = 50\nalpha = 0.05\n \n#Initialize beta parameters\nbeta = zeros(shape=(3, 1))\n \nbeta, J_history = gradientDescent(X1, y, beta, alpha, iterations)\n \n#Predict height for a 5 yr old girl weighing 20 kgs\nheight = array([1.0,   ((5 - normalizedMean[0]) / normalizedStdDev[0]), ((20 - normalizedMean[1]) / normalizedStdDev[1])]).dot(beta)\nprint 'Predicted height of a 5yr girl with weight 20kg: %f' % (height)",
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": "Predicted height of a 5yr girl with weight 20kg: 11.079383\n"
      }
     ],
     "prompt_number": 39
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "",
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}