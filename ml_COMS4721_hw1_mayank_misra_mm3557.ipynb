{
 "metadata": {
  "name": "ml_COMS4721_hw1_mayank_misra_mm3557"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "# Name:  Mayank Misra | Uni: mm3557 | ml_COMS4721_hw1_mayank_misra_mm3557"
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "#Preliminaries"
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "### Question 1: True or False?\n#### 1. Supervised and unsupervised learning both aim to identify classes in data (at the learning stage not prediction).\n####    Answer>>> True\n#### 2. When feature space is large, overfitting is likely.\n####    Answer>>> True\n#### 3. Overfitting can be controlled by regularization.\n####    Answer>>> True\n#### 4. Once you learn a classification model, you can use the test set to assess the model performance.\n####    Answer>>> True\n#### 5. If the performance of a classification model on the test set is poor, you can re-calibrate your model parameters to achieve a better model.\n####    Answer>>> False\n#### 6. Cross-validation is used only when one have a large training set.\n####    Answer>>> False\n#### 7. The examples in a validation set are used to train a classification model.\n####    Answer>>> False\n#### 8. To learn a regression model you can either use gradient descent or normal equations.\n####    Answer>>> True\n#### 9. Because it is straightforward to calculate in just one step, Normal equation is the preferred method when the feature space is large (e.g., 10,000 features).\n####    Answer>>> False\n#### 10. If the learning rate \u03b1 is small enough, gradient descent converges very fast.\n####    Answer>>> False\n#### 11. Ridge regression aims to increase the variance of linear regression by decreasing the bias.\n####    Answer>>> False\n#### 12. Lasso is a variant of linear regression that calculates a sparse solution.\n####    Answer>>> True\n#### 13. K-NN works only for classification.\n####    Answer>>> False\n#### 14. K-NN is a linear classification method.\n####    Answer>>> False\n#### 15. The loss function aggregates the classification/regression error on all examples.\n####    Answer>>> True"
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "### Question 2: Machine Learning Definition in Practice\n#### What are the sets E, T, and P in the case of a Recommender System? Please justify your answer and elaborate with examples.\n\n#### Recall: \u201cA computer program is said to learn from experience E with respect to some class of tasks T and performance measure P, if its performance at tasks in T, as measured by P, improves with experience E. \u201d Tom Mitchell.\n\n####   Answer>>> Recommendation systems are a subclass of information filtering system that seek to predict the 'rating' or 'preference' that user would give to an item. (Francesco Ricci and Lior Rokach and Bracha Shapira, Introduction to Recommender Systems Handbook, Recommender Systems Handbook, Springer, 2011, pp. 1-35.  http://www.inf.unibz.it/~ricci/papers/intro-rec-sys-handbook.pdf) \n\n####   In a recommendation system, the available data on the user (user matrix of purchase history, likes/dislikes, ratings, surveys, profile) is mined to predict future behavior (buying pattern, product preferences) in attempt to provide a contextual and personalized experience.  \n\n####   Experience E:  \n####   The trainging Experience is the learning simulations where predictions from the model are matched with known results. For example, a set of movie likes/dislikes can be used to model the probability of the user liking a particular movie. These experiences are used to further refine the model to make it better (it learns form its experiences).  \n\n####   Tasks T:  \n####   The act of predicting/recommending a specific user behavior is the task.   \n\n####   Performance P:  \n####   Performance of a recommendation systemis the accuracy with which the underlying model is able to predict the actual behaviour of a user, given the training datset (of user preferences and past behaviors).  This result will feedback into the user matrix and will become part of future predictions therby improving the acurracy of the model.   The training dataset is the past data on users that captures categorical and descriptive information (like/dislike, age, purchases etc)."
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "# Practical Problems"
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "## 1. Linear regression with one feature\n#### We are interested in studying the relationship between age and height (statures) in girls aged 2 to 8 years old. We think that this can be modeled with a linear regression model. We have examples (data points) for a population of 60 girls. Each example has one feature Age along with a numerical label Height. We will use the dataset girls_train.csv (derived from CDC growthchart data1). Your mission is to implement linear regression with one feature using gradient descent. You will plot the data, regression line, coefficient contours and cost function. You will finally make a prediction for a new example using your regression model and assess your out of sample mean square error."
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "### 1.1 Load & Plot\n#### a) Load the dataset girls.csv.\n#### b) Plot the distribution of the data. You should be getting a plot similar to Figure 1."
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "import numpy as np\n# include numpy libraries to facilitate reading text files like csv\nimport matplotlib as mpl\n# include the matplot libraries\nimport matplotlib.pyplot as plt\n# include plotting libraries\n\ndef getData():\n    # specify the data file that will be read\n    age, height = np.loadtxt('/Users/mayank/Dropbox/DataSets/ColumbiaUniversity/ML/hw1_all/girls_train.csv', \n                             dtype='float', \n                             comments='# The file contains a null third row.  Force loadtxt to read first two columns', \n                             delimiter=',', \n                             converters=None, \n                             skiprows=0, \n                             usecols=(0,1), \n                             unpack=True, \n                             ndmin=0)\n    print (age)\n    print (height)\n    \n    fig = plt.figure()\n    axl = fig.add_subplot(1,1,1,axisbg='white')\n    plt.plot(age, height, 'ro')\n    plt.title('Age and Stature')\n    plt.xlabel('Age in Years')\n    plt.ylabel('Height in Meters')\n    plt.show()\n\ngetData()\n\n",
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": "[ 2.    2.04  2.21  2.38  2.46  2.54  2.63  2.79  2.88  2.96  3.04  3.13\n  3.21  3.29  3.38  3.54  3.63  3.71  3.88  3.96  4.29  4.38  4.46  4.54\n  4.63  4.71  4.88  4.96  5.04  5.13  5.21  5.29  5.38  5.46  5.54  5.63\n  5.71  5.79  5.88  5.96  6.04  6.13  6.21  6.38  6.54  6.63  6.79  6.88\n  6.96  7.04  7.13  7.21  7.29  7.38  7.63  7.71  7.79  7.88  7.96  8.04]\n[ 0.87311214  0.87749175  0.87157142  0.94899744  0.89584767  0.95190833\n  0.93627413  0.89800446  0.99528907  0.96300288  0.96900715  0.92122983\n  0.87959451  0.98705683  0.99299569  0.97648481  1.01069581  1.01657883\n  1.02835032  1.07791928  1.0861266   1.00310761  0.96651891  0.98747315\n  1.11111413  1.02532875  1.03654885  1.04218237  1.14270129  1.14905168\n  1.05915598  1.02011035  1.16812988  1.03094842  1.18084504  1.1871931\n  1.04719061  1.06964808  1.20616342  1.05232262  1.21871833  1.24546059\n  1.06813207  1.08987746  1.10028513  1.12348325  1.20125449  1.30110291\n  1.28534483  1.31293576  1.29685453  1.22827864  1.13296343  1.27753855\n  1.18288605  1.22075253  1.16001227  1.17687516  1.35092955  1.35601536]\n"
      }
     ],
     "prompt_number": 1
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "####The plot of girls_train.csv looks like this:  [Age and Stature](https://pbs.twimg.com/media/BiUfDq5IgAEgENU.png)"
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "### 1.2 Gradient descent\n#### Now that the data is loaded and you know how it looks like, find a regression model of the form: Height=\u03b20 +\u03b21 \u00d7Age\n#### a) Implement Gradient Descent to find the \u03b2\u2019s of the model. Remember you need to add the vector 1 ahead of your data matrix. Also, make sure you update the parameters \u03b2\u2019s simultaneously. Use a learning rate alpha = 0.05 and #iterations = 1500.\n#### b) What is the mean square error of your regression model on the training set?\n"
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "# include numpy libraries to facilitate reading text files like csv\nimport numpy as np\nfrom numpy import loadtxt, zeros, ones, array, linspace, logspace\n# include the matplot libraries\nimport matplotlib as mpl\n# include plotting libraries\nimport matplotlib.pyplot as plt\nfrom pylab import scatter, show, title, xlabel, ylabel, plot, contour\n\n#Load the dataset\ndata = loadtxt('/Users/mayank/Dropbox/DataSets/ColumbiaUniversity/ML/hw1_all/girls_train.csv',\n                             dtype='float', \n                             comments='# The file contains a null third row.  Force loadtxt to read first two columns', \n                             delimiter=',', \n                             #converters=None, \n                             #skiprows=0, \n                             usecols=(0,1), \n                             unpack=True, \n                             ndmin=0\n                             ) \n \n#Initialize useful parameters for regression \ndef getCost(X, y, beta):\n\n    #Number of training samples\n    m = y.size\n \n    #calculate the cost of a particular choice of beta\n    predictedValue = X.dot(beta).flatten()\n \n    #calculate squared error\n    sqErrors = (predictedValue - y) ** 2\n    \n    J = (1.0 / (2 * m)) * sqErrors.sum()\n    \n    return J\n \n#Compute gradient descent (GD) to infer beta. GD steps = numberIterations  and learning rate = alpha\ndef gradientDescent(X, y, beta, alpha, numberIterations):\n    # number of training examples\n    m = y.size\n    J_history = zeros(shape=(numberIterations, 1))\n \n    for i in range(numberIterations):\n \n        predictedValue = X.dot(beta).flatten()\n \n        sumErrorsCol1 = (predictedValue - y) * X[:, 0]\n        sumErrorsCol2 = (predictedValue - y) * X[:, 1]\n \n        beta[0][0] = beta[0][0] - alpha * (1.0 / m) * sumErrorsCol1.sum()\n        beta[1][0] = beta[1][0] - alpha * (1.0 / m) * sumErrorsCol2.sum()\n \n        J_history[i, 0] = getCost(X, y, beta)\n \n    return beta, J_history\n \nX = data[:, 0]\ny = data[:, 1]\n \n \n#number of training samples\nm = y.size\n \n#Add a column of ones to X \nX1 = ones(shape=(m, 2))\nX1[:, 1] = X\n \n#Initialize beta parameters\nbeta = zeros(shape=(2, 1))\n \n#initialize gradient descent parameters\nnumberIterations = 1500\nalpha = 0.05\n \n#compute and display initial cost\nprint 'Initial cost is %f' % getCost(X1, y, beta)\n \nbeta, J_history = gradientDescent(X1, y, beta, alpha, numberIterations)\nprint beta\n\n",
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": "Initial cost is 1.232898\n[[-0.02288278]\n [ 1.03139807]]"
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": "\n"
      }
     ],
     "prompt_number": 36
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "### 1.3 Plot the regression line, contours and bowl function"
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "# include numpy libraries to facilitate reading text files like csv\nimport numpy as np\nfrom numpy import loadtxt, zeros, ones, array, linspace, logspace\n# include the matplot libraries\nimport matplotlib as mpl\n# include plotting libraries\nimport matplotlib.pyplot as plt\nfrom pylab import scatter, show, title, xlabel, ylabel, plot, contour\n\n#Load the dataset\ndata = loadtxt('/Users/mayank/Dropbox/DataSets/ColumbiaUniversity/ML/hw1_all/girls_train.csv',\n                             dtype='float', \n                             comments='# The file contains a null third row.  Force loadtxt to read first two columns', \n                             delimiter=',', \n                             #converters=None, \n                             #skiprows=0, \n                             usecols=(0,1), \n                             unpack=True, \n                             ndmin=0\n                             ) \n \n#Initialize useful parameters for regression \ndef getCost(X, y, beta):\n\n    #Number of training samples\n    m = y.size\n \n    #calculate the cost of a particular choice of beta\n    predictedValue = X.dot(beta).flatten()\n \n    #calculate squared error\n    sqErrors = (predictedValue - y) ** 2\n    \n    J = (1.0 / (2 * m)) * sqErrors.sum()\n \n    return J\n \n#Compute gradient descent (GD) to infer beta. GD steps = numberIterations  and learning rate = alpha\ndef gradientDescent(X, y, beta, alpha, numberIterations):\n    # number of training examples\n    m = y.size\n    J_history = zeros(shape=(numberIterations, 1))\n \n    for i in range(numberIterations):\n \n        predictedValue = X.dot(beta).flatten()\n \n        sumErrorsCol1 = (predictedValue - y) * X[:, 0]\n        sumErrorsCol2 = (predictedValue - y) * X[:, 1]\n \n        beta[0][0] = beta[0][0] - alpha * (1.0 / m) * sumErrorsCol1.sum()\n        beta[1][0] = beta[1][0] - alpha * (1.0 / m) * sumErrorsCol2.sum()\n \n        J_history[i, 0] = getCost(X, y, beta)\n \n    return beta, J_history\n \nX = data[:, 0]\ny = data[:, 1]\n \n \n#number of training samples\nm = y.size\n \n#Add a column of ones to X \nX1 = ones(shape=(m, 2))\nX1[:, 1] = X\n \n#Initialize beta parameters\nbeta = zeros(shape=(2, 1))\n \n#initialize gradient descent parameters\nnumberIterations = 1500\nalpha = 0.05\n \n#compute beta\n \nbeta, J_history = gradientDescent(X1, y, beta, alpha, numberIterations)\n\n\n\n## PLOTS \n\n#Plot the data\ndef getData():\n    # specify the data file that will be read\n    age, height = np.loadtxt('/Users/mayank/Dropbox/DataSets/ColumbiaUniversity/ML/hw1_all/girls_train.csv', \n                             dtype='float', \n                             comments='# The file contains a null third row.  Force loadtxt to read first two columns', \n                             delimiter=',', \n                             converters=None, \n                             skiprows=0, \n                             usecols=(0,1), \n                             unpack=True, \n                             ndmin=0)\n    fig = plt.figure()\n    axl = fig.add_subplot(1,1,1,axisbg='white')\n    plt.plot(age, height, 'ro')\n    plt.title('Age and Stature')\n    plt.xlabel('Age in Years')\n    plt.ylabel('Height in Meters')\n    plt.show()\n\ngetData()\n\n#Plot the results\nresult = X1.dot(beta).flatten()\nplot(data[:, 0], result)\nshow()\n \n#plot values of beta\n#initialize matrix space for beta and cost values.  \nbeta0 = linspace(-10, 10, 100)\nbeta1 = linspace(-1, 4, 100)\nJ_beta0beta1 = zeros(shape=(beta0.size, beta1.size))\n#Fill the matrix J_beta0beta1\nfor t1, element in enumerate(beta0):\n    for t2, element2 in enumerate(beta1):\n        iBeta = zeros(shape=(2, 1))\n        iBeta[0][0] = element\n        iBeta[1][0] = element2\n        J_beta0beta1[t1, t2] = getCost(X1, y, iBeta)\n \n#graph a contour plot\nJ_beta0beta1 = J_beta0beta1.T\ncontour(beta0, beta1, J_beta0beta1, logspace(-2, 3, 20))\nxlabel('beta_0')\nylabel('beta_1')\nscatter(beta[0][0], beta[1][0])\nshow()",
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 37
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "### 1.4 Testing your model and Making a prediction for a new example"
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "# include numpy libraries to facilitate reading text files like csv\nimport numpy as np\nfrom numpy import loadtxt, zeros, ones, array, linspace, logspace\n# include the matplot libraries\nimport matplotlib as mpl\n# include plotting libraries\nimport matplotlib.pyplot as plt\nfrom pylab import scatter, show, title, xlabel, ylabel, plot, contour\n\n#Load the dataset\ndata = loadtxt('/Users/mayank/Dropbox/DataSets/ColumbiaUniversity/ML/hw1_all/girls_train.csv',\n                             dtype='float', \n                             comments='# The file contains a null third row.  Force loadtxt to read first two columns', \n                             delimiter=',', \n                             #converters=None, \n                             #skiprows=0, \n                             usecols=(0,1), \n                             unpack=True, \n                             ndmin=0\n                             ) \n \n#Initialize useful parameters for regression \ndef getCost(X, y, beta):\n\n    #Number of training samples\n    m = y.size\n \n    #calculate the cost of a particular choice of beta\n    predictedValue = X.dot(beta).flatten()\n \n    #calculate squared error\n    sqErrors = (predictedValue - y) ** 2\n    \n    J = (1.0 / (2 * m)) * sqErrors.sum()\n \n    return J\n \n#Compute gradient descent (GD) to infer beta. GD steps = numberIterations  and learning rate = alpha\ndef gradientDescent(X, y, beta, alpha, numberIterations):\n    # number of training examples\n    m = y.size\n    J_history = zeros(shape=(numberIterations, 1))\n \n    for i in range(numberIterations):\n \n        predictedValue = X.dot(beta).flatten()\n \n        sumErrorsCol1 = (predictedValue - y) * X[:, 0]\n        sumErrorsCol2 = (predictedValue - y) * X[:, 1]\n \n        beta[0][0] = beta[0][0] - alpha * (1.0 / m) * sumErrorsCol1.sum()\n        beta[1][0] = beta[1][0] - alpha * (1.0 / m) * sumErrorsCol2.sum()\n \n        J_history[i, 0] = getCost(X, y, beta)\n \n    return beta, J_history\n \nX = data[:, 0]\ny = data[:, 1]\n \n \n#number of training samples\nm = y.size\n \n#Add a column of ones to X \nX1 = ones(shape=(m, 2))\nX1[:, 1] = X\n \n#Initialize beta parameters\nbeta = zeros(shape=(2, 1))\n \n#initialize gradient descent parameters\nnumberIterations = 1500\nalpha = 0.05\n\n\n#Predict height for girl aged 4.5\n\npredict1 = array([4.5, 1]).dot(beta).flatten()\nprint 'For a girl of age = 4.5, the prediction for height is %f' % (predict1)\n",
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": "For a girl of age = 4.5, the prediction for height is 0.000000\n"
      }
     ],
     "prompt_number": 28
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "## 2. Linear regression with multiple features"
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "#### In this problem, you will work on linear regression with multiple features using gradient descent and the normal equation. You will also study the relationship between the risk function, the convergence of gradient descent, and the learning rate. We will use the dataset girls age weight height 2 8.csv (derived from CDC growthchart data)."
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": "#### 2.1 Data Preparation & Normalization"
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": "",
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}